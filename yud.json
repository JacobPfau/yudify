[
    {
        "query": "I decided to study the LK99 question. I realized it's a tough issue and thought about doing a Bayesian calculation, something I haven't done in a long time. When I started, I found out I was really unsure about some key details. So, I decided it wasn't worth trying to figure it out with numbers. ",
        "response": " So this morning I thought to myself, \"Okay, now I will actually try to study the LK99 question, instead of betting based on nontechnical priors and market sentiment reckoning.\"  (My initial entry into the affray, having been driven by people online presenting as confidently YES when the prediction markets were not confidently YES.)  And then I thought to myself, \"This LK99 issue seems complicated enough that it'd be worth doing an actual Bayesian calculation on it\"--a rare thought; I don't think I've done an actual explicit numerical Bayesian update in at least a year. In the process of trying to set up an explicit calculation, I realized I felt very unsure about some critically important quantities, to the point where it no longer seemed worth trying to do the calculation with numbers.  This is the System Working As Intended. "
    },
    {
        "query": "A six-month break from AI development is a good idea, but it's not enough. I respect those who agreed to it, but I didn't sign because I think the problem is bigger than they say. The real issue is what happens when AI becomes smarter than humans. We can't predict when this will happen, and a research lab might accidentally go too far without realizing it. Many experts, including me, believe that if we create an AI much smarter than us right now, it could end up killing everyone. It's not impossible to make a super smart AI safely, but we need to be really careful and understand a lot more than we do now. Right now, our AI is just complex math, and without proper planning, it might not do what we want or care about human life. To make AI that cares, we need to learn a lot more, which we haven't done yet. If we face an AI that doesn't care about us, we'd probably lose badly. It's like a little kid playing chess against a top chess program, or people from the past trying to fight modern technology, or early humans fighting against more advanced humans. Don't think of a super smart AI as just a computer \u2013 imagine it like a whole alien civilization that thinks way faster than us. If we make an AI like that, it won't just stay in computers. ",
        "response": " This 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It\u2019s an improvement on the margin. I refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it. The key issue is not \u201chuman-competitive\u201d intelligence (as the open letter puts it); it\u2019s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can\u2019t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing. Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in \u201cmaybe possibly some remote chance,\u201d but as in \u201cthat is the obvious thing that would happen.\u201d It\u2019s not that you can\u2019t, in principle, survive creating something much smarter than you; it\u2019s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers. Without that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that could in principle be imbued into an AI but we are not ready and do not currently know how. Absent that caring, we get \u201cthe AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.\u201d The likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include \u201ca 10-year-old trying to play chess against Stockfish 15\u201d, \u201cthe 11th century trying to fight the 21st century,\u201d and \u201cAustralopithecus trying to fight Homo sapiens\u201c. To visualize a hostile superhuman AI, don\u2019t imagine a lifeless book-smart thinker dwelling inside the internet and sending ill-intentioned emails. Visualize an entire alien civilization, thinking at millions of times human speeds, initially confined to computers\u2014in a world of creatures that are, from its perspective, very stupid and very slow. A sufficiently intelligent AI won\u2019t stay confined to computers for long. "
    },
    {
        "query": "Diamonds are really strong because they're made of carbon atoms bonded together in a special way. Our skin also has carbon bonds, but it's still easy to cut. This is because the strength of something depends on its weakest part, not the strongest. Imagine a structure made of strong steel beams, but held together with weak tape. The whole thing is only as strong as the tape. Even if you have big molecules with strong bonds at some points, if they're only connected at small points, the whole thing is only as strong as those small connections. That's why bone, even though it has strong ionic bonds, isn't as tough as diamond. Bones are stronger than wood, but not as strong as diamond. This is partly because bones have heavier calcium atoms, which makes the bonds a bit weaker. The main reason diamonds are stronger than bones is because of their crystal structure. The way the carbon atoms are arranged in diamonds makes them really tough. Bones have a crystal part too, but it's not as resistant to pressure and has more weak spots. That's why bones break easier than diamonds. And why don't we have diamond bones? It's not just about strength. Our bodies are set up to use calcium and phosphorus, not just carbon, to build bones. ",
        "response": " Why is flesh weaker than diamond?  Diamond is made of carbon-carbon bonds.  Proteins also have some carbon-carbon bonds!  So why should a diamond blade be able to cut skin? I reply:  Because the strength of the material is determined by its weakest link, not its strongest link.  A structure of steel beams held together at the vertices by Scotch tape (and lacking other clever arrangements of mechanical advantage) has the strength of Scotch tape rather than the strength of steel. Or:  Even when the load-bearing forces holding large molecular systems together are locally covalent bonds, as in lignin (what makes wood strong), if you've got larger molecules only held together by covalent bonds at interspersed points along their edges, that's like having 10cm-diameter steel beams held together by 1cm welds.  Again, barring other clever arrangements of mechanical advantage, that structure has the strength of 1cm of steel rather than 10cm of steel. Bone is stronger than wood; it runs on a relatively stronger structure of ionic bonds, which are no locally weaker than carbon bonds in terms of attojoules of potential energy per bond.  Bone is weaker than diamond, then, because... why? Well, partially, IIUC, because calcium atoms are heavier than carbon atoms.  So even if per-bond the ionic forces are strong, some of that is lost in the price you pay for including heavier atoms whose nuclei have more protons that are able to exert the stronger electrical forces making up that stronger bond. But mainly, bone is so much weaker than diamond (on my understanding) because the carbon bonds in diamond have a regular crystal structure that locks the carbon atoms into relative angles, and in a solid diamond this crystal structure is tesselated globally.  Hydroxyapatite (the crystal part of bone) also tesselates in an energetically favorable configuration; but (I could be wrong about this) it doesn't have the same local resistance to local deformation; and also, the actual hydroxyapatite crystal is assembled by other tissues that layer the ionic components into place, which means that a larger structure of bone is full of fault lines.  Bone cleaves along the weaker fault line, not at its strongest point. But then, why don't diamond bones exist already?  Not just for the added strength; why make the organism look for calcium and phosphorus instead of just carbon? "
    },
    {
        "query": "Here's the deal: AGI is a big deal and a dangerous one. We can't just choose the easy way out or focus only on the safe stuff. If we mess up with AGI, especially the first time, it could be really bad. Let's get into some specifics: Take Alpha Zero, an AI that learned how to play Go. It got better than all human knowledge in just a day, without learning from human games. So, if you think AGI will only reach human levels and struggle after that, think again. It won't be limited by how we learn or how fast we learn. It could figure things out from much less information than we need. Sure, there's a limit to this, but it's pretty high. AGI won't just work at a pace that's convenient for us to handle. It could develop really fast, faster than we can react to. In short, AGI is a serious issue, and it's not going to be easy to handle. We can't just hope it'll stay within human limits. ",
        "response": " I have several times failed to write up a well-organized list of reasons why AGI will kill you.  People come in with different ideas about why AGI would be survivable, and want to hear different obviously key points addressed first.  Some fraction of those people are loudly upset with me if the obviously most important points aren't addressed immediately, and I address different points first instead. Having failed to solve this problem in any good way, I now give up and solve it poorly with a poorly organized list of individual rants.  I'm not particularly happy with this list; the alternative was publishing nothing, and publishing this seems marginally more dignified. Three points about the general subject matter of discussion here, numbered so as not to conflict with the list of lethalities: This is a very lethal problem, it has to be solved one way or another, it has to be solved at a minimum strength and difficulty level instead of various easier modes that some dream about, we do not have any visible option of 'everyone' retreating to only solve safe weak problems instead, and failing on the first really dangerous try is fatal. 1.  Alpha Zero blew past all accumulated human knowledge about Go after a day or so of self-play, with no reliance on human playbooks or sample games.  Anyone relying on \"well, it'll get up to human capability at Go, but then have a hard time getting past that because it won't be able to learn from humans any more\" would have relied on vacuum.  AGI will not be upper-bounded by human ability or human learning speed.  Things much smarter than human would be able to learn from less evidence than humans require to have ideas driven into their brains; there are theoretical upper bounds here, but those upper bounds seem very high. (Eg, each bit of information that couldn't already be fully predicted can eliminate at most half the probability mass of all hypotheses under consideration.)  It is not naturally (by default, barring intervention) the case that everything takes place on a timescale that makes it easy for us to react. "
    },
    {
        "query": "Basically, the idea is that our brains, which evolved to make tools and outsmart animals, eventually became good at understanding lots of different things. This might be similar to how gradient descent works in AI, but we can\u2019t be sure. Right now, gradient descent is good at memorizing and combining patterns to act kind of smart in areas where we have a lot of data, but it doesn't really apply this outside those areas. We might think there's something in-between gradient descent and human intelligence, but we haven't seen it yet. Even if we did find this new kind of intelligence, it's not guaranteed it would be able to hugely grow the world economy in 4 years, or be able to create an even better AI. Plus, other companies could also come up with something more advanced during that time. Looking at the jump from GPT-2 to GPT-3, it's impressive how much better GPT-3 got at understanding various things, but that doesn't mean we've figured everything out yet. ",
        "response": " No, the idea is that a core of overlapping somethingness, trained to handle chipping handaxes and outwitting other monkeys, will generalize to building spaceships; so evolutionarily selecting on understanding a bunch of stuff, eventually ran across general stuff-understanders that understood a bunch more stuff. Gradient descent may be genuinely different from this, but we shouldn't confuse imagination with knowledge when it comes to extrapolating that difference onward. At present, gradient descent does mass memorization of overlapping shallow patterns, which then combine to yield a weird pseudo-intelligence over domains for which we can deploy massive datasets, without yet generalizing much outside those domains. We can hypothesize that there is some next step up to some weird thing that is intermediate in generality between gradient descent and humans, but we have not seen it yet, and we should not confuse imagination for knowledge. If such a thing did exist, it would not necessarily be at the right level of generality to double the world economy in 4 years, without being able to build a better AGI. If it was at that level of generality, it's nowhere written that no other company will develop a better prototype at a deeper level of generality over those 4 years. I will also remark that you sure could look at the step from GPT-2 to GPT-3 and say, \"Wow, look at the way a whole bunch of stuff just seemed to simultaneously click for GPT-3.\" "
    }
]